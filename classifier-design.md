# Beginning
The two Python scripts, `train.py` and `classify.py`, work together as part of a text classification pipeline.

## Classifier structure:
- the directory `model` for storing models files (`bow_vectorizer.pkl`, `multilabel_binarizer.pkl`, `multilabel_classification_model.pkl`). They are going to be generated by the `train.py`
- the directory `explanation` for storing explanations models files (LIME doesn't work well with OneVsRestClassifier)


## Components of the classifier

The TV show genre classifier consists of the following main components:

- **Training data**: a database containing TV show descriptions and their associated genres. This data is used to train the classifier.

- **Data preprocessing functions**: preprocessing is crucial for cleaning and preparing the text data. The classifier uses various functions for this purpose, including lemmatization, stopword removal, and text cleaning.

- **BoW model**: the Bag of Words (BoW) model is a critical feature extraction technique. It represents text data numerically by considering the frequency of words within a document.

- **MultiLabelBinarizer**: this component is used to convert genre labels into binary vectors. It handles multi-label classification, allowing a TV show to belong to multiple genres.

- **Classification model**: the heart of the classifier is the classification model, which uses a logistic regression algorithm. It predicts the genres of TV shows based on their descriptions.

- **Model evaluation**: the classifier evaluates its performance by measuring accuracy. This is important to ensure the quality of predictions.


## The text classification process

### 1. Data preparation

Before any text classification can take place, it is essential to preprocess the text data. This preprocessing typically involves the following steps:

- **Tokenization**: the text is split into individual words or tokens.
- **Lowercasing**: all words are converted to lowercase to ensure case insensitivity.
- **Stopword Removal**: common and uninformative words like "the," "and," "in" are removed from the text.
- **Lemmatization or Stemming**: words are reduced to their base or root form (e.g., "running" becomes "run").
- **Cleaning**: removal of special characters, punctuation, and HTML tags.

#### Term Frequency (TF)

In the context of the BoW model, we use a mathematical concept called "Term Frequency" (TF) to represent the importance of a word within a document. TF is calculated as:

\[TF(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}\]

Where:
- \(t\) is a term (word) in the document.
- \(d\) is the document in which the term is counted.

TF measures how frequently a term appears in a document relative to the document's total terms. A higher TF indicates that the term is more important or relevant within that document.

#### The BoW Process

1. **Vocabulary Creation**: a unique vocabulary is created by collecting all the distinct terms from the entire corpus of text documents. Each term is assigned a unique index.

2. **Document-Term Matrix (DTM)**: for each document in the corpus, a vector is created where each dimension represents a unique term from the vocabulary. The value of each dimension in the vector is the TF of the corresponding term in the document.

3. **Sparsity**: the DTM is typically sparse because most terms do not appear in most documents. This is why the BoW model often uses techniques like Count Vectorization or TF-IDF (Term Frequency-Inverse Document Frequency) to handle sparsity.

## The Bag of Words (BoW) model

The Bag of Words (BoW) model is central to the text classification process.

- **Vocabulary creation**: the classifier builds a unique vocabulary by collecting all distinct terms (words) from the TV show descriptions in the training data. Each term is assigned a unique index.

- **Document-Term Matrix (DTM)**: for each TV show description, the classifier creates a vector where each dimension represents a unique term from the vocabulary. The value of each dimension in the vector is the Term Frequency (TF) of the corresponding term in the description.

- **Term Frequency (TF)**: TF measures how frequently a term appears in a TV show description relative to the total terms in that description. It is a critical concept for the BoW model.

- **Sparsity**: the DTM is often sparse because most terms do not appear in most descriptions. Techniques like Count Vectorization or TF-IDF are used to handle this sparsity.

## The Classification Process

The TV show genre classification process proceeds as follows:

1. **Data preparation**: the program (`train.py`) reads TV show descriptions and genres from an SQLite database. Descriptions are cleaned, and noisy data is removed.

2. **BoW feature extraction**: the descriptions are transformed into numerical features using the BoW model. This creates a DTM for the descriptions.

3. **Model training**: the classifier uses logistic regression for multi-label classification. It trains the model on the DTM and the associated genre labels.

4. **Model evaluation**: the classifier evaluates its accuracy by making predictions on a test set and comparing them to the ground truth.

5. **Model saving**: the trained model, MultiLabelBinarizer, and CountVectorizer are saved to files for future use.

### 3. Classification

Once the text data is converted into numerical features, a classification algorithm is applied to predict the category or label for a given document. Common classification algorithms include Logistic Regression, Naive Bayes, Support Vector Machines, and Neural Networks. The choice of classifier depends on the specific problem and dataset.

#### Math Concept: Logistic Regression

Logistic Regression is a commonly used classification algorithm. It models the probability of a document belonging to a particular category using the logistic function:

\[P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_nx_n)}}\]

Where:
- \(P(y = 1)\) is the probability that the document belongs to category 1.
- \(\beta_0, \beta_1, \beta_2, \ldots, \beta_n\) are the model coefficients.
- \(x_1, x_2, \ldots, x_n\) are the features (in our case, the TF values of terms).

The logistic function maps any real-valued number to the range \([0, 1]\), which can be interpreted as a probability. If \(P(y = 1) > 0.5\), the document is classified into category 1; otherwise, it is classified into category 0.


## Usage of the classifier

1. **Training the classifier (optional)**:

    if you haven't already trained the classifier, you can use `train.py` to do so. Run the following command to train the classifier with your data:

    ```shell
    python train.py --training-data tvmaze.sqlite
    ```
    It will save models into the directory `model`


2. **Classify TV show descriptions**:

    To classify TV show descriptions, use `classify.py`. Run the following command:

    ```shell
    python classify.py --input-file classify-input.txt --output-json-file classifying-result.json --encoding UTF-8 --explanation-output-dir explanation
    ```
   
3. **Optional: generate explanations**:

    If you want to generate explanations for the predictions, you can specify an output directory using the `--explanation-output-dir` option in `classify.py`. This will create explanation files in both text and HTML formats.

    ```shell
    python classify.py --input-file your_input_description.txt --output-json-file output.json --explanation-output-dir explanation
    ```

    The explanations will help you understand why the program made specific genre predictions.